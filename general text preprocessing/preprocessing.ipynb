{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d274bb8f-1851-4eaf-9981-bda609a088e7",
      "metadata": {
        "id": "d274bb8f-1851-4eaf-9981-bda609a088e7"
      },
      "source": [
        "## tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21b2b64a-710d-461e-9afa-b22dc830557f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21b2b64a-710d-461e-9afa-b22dc830557f",
        "outputId": "3bf812d4-12b2-4929-a0f0-645cda7d4b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bcc8210b-dd7d-43f0-86ed-8d4c20d9d152",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcc8210b-dd7d-43f0-86ed-8d4c20d9d152",
        "outputId": "1baac11a-3571-49a8-c724-341c954056b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'buger', 'in', 'BurgerKing', 'is', '12', '$', '.']\n",
            "['Am', 'i', 'going', 'to', 'be', 'tokenized', '?', 'Coooool', '.']\n"
          ]
        }
      ],
      "source": [
        "# tokenize the input sentence into words and pucntuation marks\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "input = \"The price\\t of buger \\nin BurgerKing is 12$.\\n\"\n",
        "print(word_tokenize(input))\n",
        "\n",
        "input = \"Am i going to be tokenized ? Coooool .\\n\"\n",
        "print(word_tokenize(input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ef7176a8-1811-494b-961e-7ad713686f7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef7176a8-1811-494b-961e-7ad713686f7b",
        "outputId": "17319c83-1d46-46b8-88b7-b37536989cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'burger', 'in', 'BurgerKing', 'is', '12$.']\n"
          ]
        }
      ],
      "source": [
        "# extract the tokens without whitespaces, new line and tabs\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "Tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "sent = \"The price\\t of burger \\nin BurgerKing is 12$.\\n\"\n",
        "print(Tokenizer.tokenize(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2444f567-a3d9-4f04-b5cd-c0ce2fa9f752",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2444f567-a3d9-4f04-b5cd-c0ce2fa9f752",
        "outputId": "014c3687-8b55-4ba0-84a3-32f76f1eff4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price of burger in BurgerKing is 12\n"
          ]
        }
      ],
      "source": [
        "# tokenization with regular expression\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "\n",
        "sent = \"The price\\t of burger \\nin BurgerKing is 12$.\\n\"\n",
        "tokens = tokenizer.tokenize(sent)\n",
        "print(' '.join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "83c0a43a-1baa-48f5-b330-2d0ee9e80806",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83c0a43a-1baa-48f5-b330-2d0ee9e80806",
        "outputId": "edd646d5-f9c0-4f92-b258-d7486394a2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'burger', 'in', 'BurgerKing', 'is', '12', '$.']\n"
          ]
        }
      ],
      "source": [
        "# extract the tokens in the form of Alphabetic and Non-Alphabetic character\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "Tokenizer = WordPunctTokenizer()\n",
        "\n",
        "sent = \"The price\\t of burger \\nin BurgerKing is 12$.\\n\"\n",
        "print(Tokenizer.tokenize(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3e1d184e-2d7e-4bee-9383-fbe20286202a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e1d184e-2d7e-4bee-9383-fbe20286202a",
        "outputId": "3a461655-4d35-4359-acf5-b15bef028e77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Greetings!', 'nlp is an awsome field :)']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# tokenize the input text into sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "par = \"Greetings! nlp is an awsome field :) \"\n",
        "sent_tokenize(par)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eafeafe-2c81-4731-b390-26361cfae361",
      "metadata": {
        "id": "6eafeafe-2c81-4731-b390-26361cfae361"
      },
      "source": [
        "## normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e05842e2-4188-4bf4-8649-a85109e21881",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e05842e2-4188-4bf4-8649-a85109e21881",
        "outputId": "f1ff3450-7f7f-4b3f-a99f-84e44b7e1f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the price of burger in burgerking is 12 $ .\n"
          ]
        }
      ],
      "source": [
        "# lowecase the words within the input sentence\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sent = \"The price\\t of burger \\nin BurgerKing is 12$.\\n\"\n",
        "tokens = word_tokenize(sent)\n",
        "tokens = [word.lower() for word in tokens]\n",
        "print(' '.join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "25d53c47-cfb8-4987-868c-28ccfe9b4945",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25d53c47-cfb8-4987-868c-28ccfe9b4945",
        "outputId": "99c3f837-e3b3-4c86-b304-9bbadf155f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price of burger in BurgerKing is\n"
          ]
        }
      ],
      "source": [
        "# remove the non-alphabetic words\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sent = \"The price\\t of burger \\nin BurgerKing is 12$.\\n\"\n",
        "tokens = word_tokenize(sent)\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "print(' '.join(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98b0967-6347-4d9b-97f9-4abc3e2df7a9",
      "metadata": {
        "id": "b98b0967-6347-4d9b-97f9-4abc3e2df7a9"
      },
      "source": [
        "## lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4134d1a4-d124-415a-bfdf-98a17fea5be0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4134d1a4-d124-415a-bfdf-98a17fea5be0",
        "outputId": "e83305e3-b500-41de-beec-4c77c73357a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be have do languages cities mice\n",
            "been had done language city mouse\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sent = \"been had done languages cities mice\"\n",
        "sent = word_tokenize(sent)\n",
        "\n",
        "tokens = []\n",
        "for word in sent:\n",
        "    tokens.append(lemmatizer.lemmatize(word,  pos = \"v\"))\n",
        "\n",
        "print(' '.join(tokens))\n",
        "\n",
        "tokens = []\n",
        "for word in sent:\n",
        "    tokens.append(lemmatizer.lemmatize(word,  pos = \"n\"))\n",
        "\n",
        "print(' '.join(tokens))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83eadca6-07f9-492c-9b5e-a909cc3afe0a",
      "metadata": {
        "id": "83eadca6-07f9-492c-9b5e-a909cc3afe0a"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b9217fa9-e8ae-4ff1-88a4-b4c0ada86d66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9217fa9-e8ae-4ff1-88a4-b4c0ada86d66",
        "outputId": "9aa425b2-0964-4fb5-ab0c-e456ca900659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are sever type of stem algorithm .\n",
            "there are sever type of stem algorithm .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "sent = \"There are several types of stemming algorithms.\"\n",
        "sent = word_tokenize(sent)\n",
        "\n",
        "tokens = []\n",
        "for word in sent:\n",
        "    tokens.append(porter.stem(word))\n",
        "print(' '.join(tokens))\n",
        "\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "sent = \"There are several types of stemming algorithms.\"\n",
        "sent = word_tokenize(sent)\n",
        "\n",
        "tokens = []\n",
        "for word in sent:\n",
        "    tokens.append(snowball.stem(word))\n",
        "print(' '.join(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5699173e-588f-4ac3-896d-05e510066dd1",
      "metadata": {
        "id": "5699173e-588f-4ac3-896d-05e510066dd1"
      },
      "source": [
        "## stopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "44002ef5-08e0-4510-b2e3-36e91ad07083",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44002ef5-08e0-4510-b2e3-36e91ad07083",
        "outputId": "b2bb2933-aa61-4b36-c9e0-b99dbf68c961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['several', 'types', 'stemming', 'algorithms', '.']\n",
            "['several', 'stemming', 'algorithms', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)\n",
        "\n",
        "sent = \"There are several types of stemming algorithms.\"\n",
        "sent = word_tokenize(sent)\n",
        "tmp = [word.lower() for word in sent]\n",
        "tmp = [word for word in tmp if not word in stop_words]\n",
        "print(tmp)\n",
        "\n",
        "\n",
        "stop_words.extend([\"types\"])\n",
        "tmp = [word.lower() for word in sent]\n",
        "tmp = [word for word in tmp if not word in stop_words]\n",
        "print(tmp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}